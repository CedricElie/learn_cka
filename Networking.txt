======================   NETWORKING 	===========



How to make linux act as a router

> ip route add 192.168.1.0/24 via 192.168.2.6

192.168.1.0/24  - target netowkr
192.168.2.6 exit interface


cat /proc/sys/net/ipv4/ip_forward
Set this to 1 to see pings go through from one network to another

Persist the changes in the /etc/sysctl.conf file
-
net.ipv4.ip_forward = 1
-

> ip link
To list and modify interface on a host

> ip addr
To see the ip add assigned

> ip addr add 192.168.1.10/24 dev eth0
to set the ip address of an interface

> ip route
To view the routing table

> ip route add 192.168.1.0/24 via 192.168.2.1
To create a routing entry

> cat /proc/sys/net/ipv4/ip_forward
To view is host is configured as a router

---------------  Prerequisite DNS	--------------

In case there is a conflict between an entry in local /etc/hosts file and the DNS server, you can alther the file /etc/nsswitch.conf
--
hosts:		files, dns
--


-----------		Prerequisite CoreDNS		---------


https://github.com/kubernetes/dns/blob/master/docs/specification.md

https://coredns.io/plugins/kubernetes/


------------	Prerequisite Network Namespaces		----------

Create a new network namespace
> ip netns add red
> ip netns add blue

To view Ip namespaces
> ip netns

To view network address within a network namespace
> ip netns exec red ip link
or
> ip -n red link


To create ns network interfaces
> ip link add veth-red type veth peer name veth-blue

Attach the veth-red to the red namespace
> ip link set veth-red netns red

Attach IP addresses to each of the namespaces
> ip link set veth-blue netns blue

Assing IP addresses to the containers
> ip -n red addr add 192.168.15.1 dev veth-red
> ip -n blue addr add 192.168.15.2 dev veth-blue

Bring the interfaces up in the two namespaces
> ip -n red link set veth-red up
> ip -n blue link set veth-blue up

You can list arp tables on namespaces, but the host arp does not know about these namespaces

> ip netns exec red arp
> ip netns exec blue arp
> arp


Creating a virtual network and virtual switch to connect many namespaces

> ip link add v-net-0 type bridge

it's currently down, you can bring it up with the following command
> ip link set dev v-net-0 up

Connect the namespaces to the network, let's first delete the veth-red link
> ip -n red link del veth-red

Recreate the cables
> ip link add veth-red type veth peer name veth-red-br
> ip link add veth-blue type veth peer name veth-blue-br

Attach cables to namespaces and to the switch 
> ip link set veth-red netns red
> ip link set veth-red-br master v-net-0

Do same with the blue
> ip link set veth-blue netns blue
> ip link set veth-blue-br master v-net-0


Attach IP addresses to those interfaces
> ip -n red addr add 192.168.15.1 dev veth-red
> ip -n red addr add 192.168.15.2 dev veth-b1 

bring the interfaces up
> ip -n red link set veth-red up
> ip -n blue link set veth-blue up

At this level machines are not pingeable to one another, add an IP addres v-net-0 to make it reachable
> ip addr add 192.168.15.5/24 dev v-net-0



Routing in namespaces
> ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5

Adding NAT fonctionnality to the host
> iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -J MASQUERADE

Now you can ping from a namespace to containers connected to v-net-0
> ip netns exec blue ping 192.168.1.3

A default gateway can also be added
> ip netns exec blue ip route add default via 192.168.15.5

Now you can ping to the outside world
> ip netns exec blue ping 8.8.8.8

FAQ
While testing the Network Namespaces, if you come across issues where you can’t ping one namespace from the other, make sure you set the NETMASK while setting IP Address. ie: 192.168.1.10/24

ip -n red addr add 192.168.1.10/24 dev veth-red

Another thing to check is FirewallD/IP Table rules. Either add rules to IP Tables to allow traffic from one namespace to another. Or disable IP Tables all together (Only in a learning environment).


---------- docker networking ----------


None network (container is not connected to any network and connot reach the outside world)
  > docker run --network none nginx  

Host Network (container is attached on host network and port )
	> docker run --network host nginx

bridge network 
when docker is installed, it creates a default bridge network
> docker network ls

Docker calls this bridge network as "bridget", but on host you can view it as docker0 using:
> ip link

docker0 bridge interface network is naturally DOWN

> ip addr
docker0 ip is given as 172.17.0.1/24

Run the ip
> ip netns to view network namespaces


----------------	Prerequisite on CNI		-------------

Container Networking Interface

In the upcoming labs, we will work with Network Addons. This includes installing a network plugin in the cluster. While we have used weave-net as an example, please bear in mind that you can use any of the plugins which are described here:

https://kubernetes.io/docs/concepts/cluster-administration/addons/

https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model

In the CKA exam, for a question that requires you to deploy a network addon, unless specifically directed, you may use any of the solutions described in the link above.

However, the documentation currently does not contain a direct reference to the exact command to be used to deploy a third party network addon.

The links above redirect to third party/ vendor sites or GitHub repositories which cannot be used in the exam. This has been intentionally done to keep the content in the Kubernetes documentation vendor neutral.

At this moment in time, there is still one place within the documentation where you can find the exact command to deploy weave network addon:

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#steps-for-the-first-control-plane-node (step 2)


---------------		Pod Networking		-----------------

CNI plugins are present in the following directory : /etc/cni/net.d/
--cni-bin-dir=/opt/cni/bin/
--cni-conf-dir=/etc/cni/net.d/
--network-plugin=cni 

To list all supported cni plugins 
> ls /opt/cni/bin

 View kubelet options
> ls /etc/cni/net.d/
10-bridge.conf

-------------	CNI Weave	------------

WeaveWorks

Deploying weave is as pods on the cluster
> kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
> kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&env.IPALLOC_RANGE=10.50.0.0/16"


-----------------	IPAM Weave	-----------

Weave IP range : 10.32.0.0/12

IP Address Management

CNI configuration has a section for IPAM
> cat /etc/cni/net.d/net-script.conf

Identify the name of the bridge network/interface created by weave on each node
> ip link (no clear solution)

What is the POD IP address range configured by weave?




---------------		Service Networking		-------------

What network range are the nodes in the cluster part of?


ip a | grep eth0 

ipcalc -b 10.2.37.12/24

iptables -L -t nat | grep db-service

To view the ip range configured for services within the cluster
> cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range

Service range 

> cat /etc/kubernetes/manifest/kube-apiserver.yaml | grep cluster-ip-range

What type of proxy is the kube-proxy configured to use

> kubectl logs <kube-proxy-pod-name> -n kube-system



------------------	DNS in Kubernetes	------------

CoreDBS is deployed as two dpods


()                <|                ()
test             web-service        web

From test you can curl to web via it's service name in the same namespace
> curl http://web-service

If they are in different namespaces (default, apps)
> curl http://web-service.apps

All services are further grouped together in a subdomain call svc
> curl http://web-service.apps.svc

All service and pods are grouped together into a root domain cluster.local by default
> curl http://web-service.apps.svc.cluster.local

This is the FQDN by the cluster

For PODS, records are not created by default, but kubernetes changes the dots in their ips into dashes
web pod at ip 10.244.2.5 becomes

> curl http://10-244-2-5.apps.pod.cluster.local


-------------- CoreDNS in Kubernetes --------

cat /etc/coredns/Corefile

To find the root domain/zone configured
> kubectl describe configmap coredns -n kube-system

kubectl edit deploy webapp

To view the root domain/zone configured for this kubernetes cluster

> kubectl describe configmap coredns -n kube-system

--------------	INGRESS	-------------


nginx controller


apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
	  name: nginx-ingress
  template:
    metadata:
	  labels:
	    name: nginx-ingress
	spec:
	  containers:
	  - name: nginx-ingress-controller
	    image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
	  args:
	    - /nginx-ingress-controller
        - --configmap=$(POD_NAMESPACE)/nginx-configuration
	  env:
	    - name: PDO_NAME
		  valueFrom:
		    fieldRef:
			  fieldPath: metadata.name
        - name: POD_NAMESPACE
		  valueFrom:
		    fieldRef:
			  fieldPath: metadata.namespace
      ports:
	    - name: http
		  containerPort: 80
		- name: https
		  containerPort: 443
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https
  selector:
    name: nginx-ingress
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration



Creating an ingress ressource

ingress-wear.yaml

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear
spec:
  backend:
    serviceName: wear-service
	servicePort: 80
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear
spec:
  rules:
  - http:
      paths:
	  - path: /wear
	    backend:
	      serviceName: wear-service
		  servicePort: 80
	- path: /watch
	    backend:
	      serviceName: watch-service
		  servicePort: 80


NB : Remember to deploy a service with the name > default-http-backend on port 80

Ingress definitions can also be done using domain names

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - host: wear.my-online-store.com
    http:
      paths:
	  - backend:
	      serviceName: wear-service
		  servicePort: 80
  - host: watch.my-online-store.com
    http:
      paths:
	  - backend:
	      serviceName: watch-service
		  servicePort: 80


Since k8s version 1.20+, it is now possible to create an Ingress resource from the imperateive way like this:-

Format - kubectl create ingress <ingress-name> --rule="host/path=service:port"

Example - kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"

Find more information and examples in the below reference link:-

https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-ingress-em- 

References:-

https://kubernetes.io/docs/concepts/services-networking/ingress

https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types


----------  Ingress - Annotations and rewrite-target  ----------

Ingress options 
https://kubernetes.github.io/ingress-nginx/examples/

Different ingress controllers have different options that can be used to customise the way it works. 
NGINX Ingress controller has many options that can be seen here. 
I would like to explain one such option that we will use in our labs. The Rewrite target option.

 
Our watch app displays the video streaming webpage at http://<watch-service>:<port>/

Our wear app displays the apparel webpage at http://<wear-service>:<port>/

We must configure Ingress to achieve the below. When user visits the URL on the left, his request should be forwarded internally to the URL on the right. 
Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forwarded users to the appropriate application in the backend. 
The applications don’t have this URL/Path configured on them:

 
http://<ingress-service>:<ingress-port>/watch –> http://<watch-service>:<port>/

http://<ingress-service>:<ingress-port>/wear –> http://<wear-service>:<port>/

 

Without the rewrite-target option, this is what would happen:

http://<ingress-service>:<ingress-port>/watch –> http://<watch-service>:<port>/watch

http://<ingress-service>:<ingress-port>/wear –> http://<wear-service>:<port>/wear

 

Notice watch and wear at the end of the target URLs. The target applications are not configured with /watch or /wear paths. 
They are different applications built specifically for their purpose, so they don’t expect /watch or /wear in the URLs. 
And as such the requests would fail and throw a 404 not found error.


To fix that we want to “ReWrite” the URL when the request is passed on to the watch or wear applications. 
We don’t want to pass in the same path that user typed in. So we specify the rewrite-target option. 
This rewrites the URL by replacing whatever is under rules->http->paths->path which happens to be /pay in this case with the value in rewrite-target. 
This works just like a search and replace function.

For example: replace(path, rewrite-target)

In our case: replace("/path","/")

 

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        backend:
          serviceName: pay-service
          servicePort: 8282
 

In another example given here, this could also be:

replace("/something(/|$)(.*)", "/$2")

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80
        path: /something(/|$)(.*)
		
---------------   ingress-wear-watch
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port:
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port:
              number: 8080
        path: /watch
        pathType: Prefix	  
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282


Creating a serice

kubectl expose deployment ingress-controller --type=NodePort --port=80 --name=ingress --dry-run=client -n ingress-space -o yaml > ingress.yaml